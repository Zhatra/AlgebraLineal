\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}

\begin{document}

\[
\boxed{\text{Sean } K = \mathbb{R} \text{ y } V = \{(x_n)_n : x_n \in \mathbb{R} \quad \forall n \in \mathbb{N}^+\}.}
\]
\[
\boxed{\text{1. Sea } p \geq 1. \text{ Demuestra que } l^p = \{(x_n)_n : \sum_{n=1}^\infty |x_n|^p < \infty\} \text{ es un subespacio vectorial de } V.}
\]
\text{Sugerencia: Prueba que para cualesquiera } a, b \in \mathbb{R} \text{ se tiene que } |a + b|^p \leq 2^p (|a|^p + |b|^p).

\section*{Demostración}

Para demostrar que \( l^p \) es un subespacio vectorial de \( V \), necesitamos probar tres propiedades:

\begin{enumerate}
    \item El vector cero está en \( l^p \).
    \item \( l^p \) es cerrado bajo la suma de vectores.
    \item \( l^p \) es cerrado bajo la multiplicación por escalares.
\end{enumerate}

\subsection*{1. Vector cero}

El vector cero en \( V \) es la secuencia \( (x_n)_n \) donde \( x_n = 0 \) para todo \( n \). Claramente, \( \sum_{n=1}^\infty |0|^p = 0 < \infty \), por lo que el vector cero está en \( l^p \).

\subsection*{2. Cerrado bajo la suma}

Supongamos que \( (a_n)_n \) y \( (b_n)_n \) son elementos de \( l^p \). Entonces, \( \sum_{n=1}^\infty |a_n|^p < \infty \) y \( \sum_{n=1}^\infty |b_n|^p < \infty \).

Queremos mostrar que \( (a_n + b_n)_n \) también está en \( l^p \).

\[
\begin{aligned}
\sum_{n=1}^\infty |a_n + b_n|^p &\leq \sum_{n=1}^\infty 2^p(|a_n|^p + |b_n|^p) \\
&= 2^p \left( \sum_{n=1}^\infty |a_n|^p + \sum_{n=1}^\infty |b_n|^p \right) \\
&< \infty
\end{aligned}
\]

La desigualdad \( |a+b|^p \leq 2^p(|a|^p + |b|^p) \) se utiliza en el primer paso, y es válida para \( p \geq 1 \).

\subsection*{3. Cerrado bajo la multiplicación por escalares}

Supongamos que \( (a_n)_n \) es un elemento de \( l^p \) y \( c \) es un escalar en \( \mathbb{R} \).

\[
\sum_{n=1}^\infty |c a_n|^p = |c|^p \sum_{n=1}^\infty |a_n|^p < \infty
\]

Por lo tanto, \( (c a_n)_n \) también está en \( l^p \).

Hemos demostrado las tres propiedades, por lo que \( l^p \) es un subespacio vectorial de \( V \).

\[
\boxed{\text{2. Prueba que el espacio } l^{\infty} = \{(x_n)_n : (x_n)_n \text{ es acotada}\} \text{ es un subespacio vectorial de } V.}
\]


Para demostrar que \( l^\infty \) es un subespacio vectorial de \( V \), necesitamos verificar tres propiedades:

\begin{enumerate}
    \item \textbf{Vector Cero}: ¿El vector cero está en \( l^\infty \)?
    \item \textbf{Cerrado bajo la Suma}: Si tomas dos vectores en \( l^\infty \), ¿su suma también está en \( l^\infty \)?
    \item \textbf{Cerrado bajo la Multiplicación por Escalares}: Si tomas un vector en \( l^\infty \) y lo multiplicas por un escalar, ¿el resultado también está en \( l^\infty \)?
\end{enumerate}

\subsection*{1. Vector Cero}

El vector cero en \( V \) es la secuencia \( (x_n)_n \) donde \( x_n = 0 \) para todo \( n \). Claramente, esta secuencia es acotada porque todos sus elementos son iguales a cero. Por lo tanto, el vector cero está en \( l^\infty \).

\subsection*{2. Cerrado bajo la Suma}

Supongamos que \( (a_n)_n \) y \( (b_n)_n \) son elementos de \( l^\infty \). Esto significa que ambas secuencias son acotadas. Podemos encontrar constantes \( M \) y \( N \) tales que:

\[
|a_n| \leq M \quad \text{y} \quad |b_n| \leq N \quad \forall n
\]

Queremos mostrar que \( (a_n + b_n)_n \) también está en \( l^\infty \).

Para cualquier \( n \), tenemos:

\[
|a_n + b_n| \leq |a_n| + |b_n| \leq M + N
\]

Por lo tanto, la secuencia \( (a_n + b_n)_n \) también es acotada y está en \( l^\infty \).

\subsection*{3. Cerrado bajo la Multiplicación por Escalares}

Supongamos que \( (a_n)_n \) es un elemento de \( l^\infty \) y \( c \) es un escalar en \( \mathbb{R} \).

Podemos encontrar una constante \( M \) tal que \( |a_n| \leq M \) para todo \( n \).

Entonces, para cualquier \( n \), tenemos:

\[
|c a_n| = |c| \cdot |a_n| \leq |c| \cdot M \quad \forall n
\]

Por lo tanto, \( (c a_n)_n \) también está en \( l^\infty \).

Hemos demostrado las tres propiedades, por lo que \( l^\infty \) es un subespacio vectorial de \( V \).


\[
\boxed{
\begin{minipage}{1\textwidth}
3. Prueba que los anteriores subespacios no son de dimensión finita. 
[Sugerencia: Piensa en sucesiones con unos y ceros.]
\end{minipage}
}
\]


\section*{Demostración para \( l^p \)}

\subsection*{Paso 1: Construcción de una familia infinita de vectores}

Consideremos la familia de vectores \( \{ e_n \} \) en \( l^p \), donde cada \( e_n \) es una sucesión que tiene un ``1'' en la \( n \)-ésima posición y ``0'' en todas las demás. Es decir,

\[
e_1 = (1, 0, 0, \ldots), \quad e_2 = (0, 1, 0, \ldots), \quad e_3 = (0, 0, 1, \ldots), \ldots
\]

\subsection*{Paso 2: Verificación de que los vectores están en \( l^p \)}

Cada uno de estos vectores \( e_n \) pertenece a \( l^p \) porque la suma de las potencias \( p \)-ésimas de sus elementos es finita. En particular, para cada \( e_n \), tenemos:

\[
\sum_{i=1}^\infty |(e_n)_i|^p = 1^p + 0 + 0 + \ldots = 1 < \infty
\]

\subsection*{Paso 3: Verificación de la independencia lineal}

Supongamos que tenemos una combinación lineal de estos vectores que es igual al vector cero:

\[
c_1 e_1 + c_2 e_2 + c_3 e_3 + \ldots = 0
\]

Esto implicaría que para cada \( n \), \( c_n \cdot 1 + 0 + 0 + \ldots = 0 \), lo que a su vez implica que \( c_n = 0 \). Por lo tanto, los vectores \( e_n \) son linealmente independientes.

\subsection*{Conclusión para \( l^p \)}

Dado que hemos encontrado una familia infinita de vectores linealmente independientes en \( l^p \), concluimos que \( l^p \) no puede ser de dimensión finita.

\section*{Demostración para \( l^\infty \)}

Para \( l^\infty \), podemos usar exactamente la misma familia de vectores \( \{ e_n \} \) y seguir los mismos pasos que en la demostración para \( l^p \).

\subsection*{Paso 1 y Paso 2}

Los vectores \( e_n \) son sucesiones acotadas, ya que todos sus elementos son 0 o 1. Por lo tanto, cada \( e_n \) está en \( l^\infty \).

\subsection*{Paso 3 y Conclusión}

La independencia lineal de los vectores \( e_n \) y la conclusión son exactamente las mismas que en el caso de \( l^p \).

Por lo tanto, \( l^\infty \) tampoco puede ser de dimensión finita.

\[
\boxed{
\begin{minipage}{0.95\textwidth}
Sea \( V \) un \( K \)-espacio vectorial y sea \( S \subset V \). Demuestra que \( S \) es un subconjunto linealmente dependiente si y sólo si \( S = \{\theta_V\} \) o bien existen vectores distintos \( v, u_1, \ldots, u_n \in S \) tal que \( v \) es combinación lineal de \( u_1, \ldots, u_n \).
\end{minipage}
}
\]


Para demostrar que \( S \) es un subconjunto linealmente dependiente si y sólo si \( S = \{\theta_V\} \) o bien existen vectores distintos \( v, u_1, \ldots, u_n \in S \) tal que \( v \) es combinación lineal de \( u_1, \ldots, u_n \), procederemos en dos partes:

\begin{enumerate}
    \item \( \Rightarrow \): Si \( S \) es linealmente dependiente, entonces \( S = \{\theta_V\} \) o existen vectores distintos \( v, u_1, \ldots, u_n \in S \) tal que \( v \) es combinación lineal de \( u_1, \ldots, u_n \).
    \item \( \Leftarrow \): Si \( S = \{\theta_V\} \) o existen vectores distintos \( v, u_1, \ldots, u_n \in S \) tal que \( v \) es combinación lineal de \( u_1, \ldots, u_n \), entonces \( S \) es linealmente dependiente.
\end{enumerate}

\textbf{Demostración}:

\begin{enumerate}
    \item \( \Rightarrow \):
    
    Supongamos que \( S \) es linealmente dependiente. Entonces, existen vectores \( v_1, v_2, \ldots, v_k \in S \) y escalares \( c_1, c_2, \ldots, c_k \) no todos cero, tales que:
    \[ c_1 v_1 + c_2 v_2 + \ldots + c_k v_k = \theta_V \]
    Sin pérdida de generalidad, supongamos que \( c_1 \neq 0 \). Entonces, podemos expresar \( v_1 \) en términos de los otros vectores:
    \[ v_1 = -\frac{c_2}{c_1} v_2 - \frac{c_3}{c_1} v_3 - \ldots - \frac{c_k}{c_1} v_k \]
    Esto muestra que \( v_1 \) es una combinación lineal de los otros vectores en \( S \). Si \( k = 1 \), entonces \( c_1 v_1 = \theta_V \), lo que implica que \( v_1 = \theta_V \) y \( S = \{\theta_V\} \).

    \item \( \Leftarrow \):

    \begin{enumerate}
        \item Si \( S = \{\theta_V\} \), entonces el conjunto es trivialmente linealmente dependiente porque cualquier escalar multiplicado por \( \theta_V \) es igual a \( \theta_V \).
        \item Supongamos que existen vectores distintos \( v, u_1, \ldots, u_n \in S \) tal que \( v \) es combinación lineal de \( u_1, \ldots, u_n \). Entonces, existen escalares \( a_1, \ldots, a_n \) tales que:
        \[ v = a_1 u_1 + \ldots + a_n u_n \]
        Esto implica que:
        \[ a_1 u_1 + \ldots + a_n u_n - v = \theta_V \]
        Dado que al menos uno de los escalares (correspondiente a \( -v \)) es diferente de cero, \( S \) es linealmente dependiente.
    \end{enumerate}
\end{enumerate}

Por lo tanto, hemos demostrado ambas direcciones, lo que completa la prueba.\\
\\ \\
\boxed{
\begin{gathered}
\text{Sea }K\text{ un campo y sea }n\in\mathbb{N}^+.\text{ Demuestra que si }\{A_1,...,A_k\}\text{ es un subconjunto linealmente} \\
\mathrm{~independiente~de~}M_{n\times n}(K),\mathrm{~entonces~}\{A_{1}^{t},...,A_{k}^{t}\}\mathrm{~también~es~linealmente~independiente}. 
\end{gathered}
}\\ \\

\textbf{Demostración:}

Supongamos que el conjunto \( \{A_{1}^{t},...,A_{k}^{t}\} \) es linealmente dependiente. Entonces, existen coeficientes \( c_1, c_2, ..., c_k \) no todos cero, tales que:
\[ c_1 A_{1}^{t} + c_2 A_{2}^{t} + ... + c_k A_{k}^{t} = 0 \]
Donde \( 0 \) es la matriz nula de \( n \times n \).

Tomando la transposición en ambos lados de la ecuación, obtenemos:
\[ (c_1 A_{1}^{t} + c_2 A_{2}^{t} + ... + c_k A_{k}^{t})^t = 0^t \]
Usando las propiedades de la transposición, la ecuación anterior se convierte en:
\[ c_1 A_1 + c_2 A_2 + ... + c_k A_k = 0 \]
Dado que se nos dice que el conjunto \( \{A_1,...,A_k\} \) es linealmente independiente, la única solución a la ecuación anterior es que todos los coeficientes \( c_1, c_2, ..., c_k \) sean cero. Sin embargo, esto contradice nuestra suposición inicial de que no todos los coeficientes son cero.

Por lo tanto, nuestra suposición inicial era incorrecta, y el conjunto \( \{A_{1}^{t},...,A_{k}^{t}\} \) es linealmente independiente.

\[ \blacksquare \]

\boxed{
\begin{aligned}
& \text{Sean }K=\mathbb{R}\text{ y }V=\{f:\mathbb{R}\to\mathbb{R}:f\text{ es función}\}.\text{ Definamos a las funciones }f:\mathbb{R}\to\mathbb{R}\text{ y }g:\mathbb{R}\to\mathbb{R} \\
& \text{por} \\
& \begin{aligned}f(x)=e^{rx}\quad & \text{y}\quad & g(x)=e^{sx},\end{aligned} \\
& \text{donde } s\neq r.\text{ Prueba que el conjunto }\{f,g\}\text{es un subconjunto linealmente independiente de }V
\end{aligned}
}  \\ \\

\textbf{Demostración:}

Para demostrar que el conjunto \( \{f, g\} \) es linealmente independiente en \( V \), debemos mostrar que si existen coeficientes \( c_1 \) y \( c_2 \) tales que:
\[ c_1 f(x) + c_2 g(x) = 0 \]
para todo \( x \in \mathbb{R} \), entonces \( c_1 = 0 \) y \( c_2 = 0 \).

Dadas las definiciones de \( f \) y \( g \), tenemos:
\[ c_1 e^{rx} + c_2 e^{sx} = 0 \]
para todo \( x \in \mathbb{R} \).

Para \( x = 0 \), la ecuación anterior se reduce a:
\[ c_1 + c_2 = 0 \]
\[ \Rightarrow c_1 = -c_2 \quad \text{(1)} \]

Ahora, derivemos la ecuación con respecto a \( x \):
\[ c_1 r e^{rx} + c_2 s e^{sx} = 0 \]
Para \( x = 0 \), obtenemos:
\[ c_1 r + c_2 s = 0 \]
\[ \Rightarrow c_1 = -\frac{s}{r} c_2 \quad \text{(2)} \]

Comparando (1) y (2), si \( c_2 \neq 0 \), entonces:
\[ -c_2 = -\frac{s}{r} c_2 \]
\[ \Rightarrow r = s \]
Pero esto contradice nuestra suposición de que \( s \neq r \). Por lo tanto, \( c_2 = 0 \). Usando (1), concluimos que \( c_1 = 0 \) también.

Por lo tanto, el conjunto \( \{f, g\} \) es linealmente independiente en \( V \).


\begin{center}
\boxed{
\begin{aligned}
&\text{Sea }V\text{ un }K-\text{espacio vectorial. Demuestra que si }\{v_1,v_2,...,v_n\}\text{ es un subconjunto linealmente}\\
&\text{independiente de }V,\text{ entonces la lista}\\
&v_1-v_2,v_2-v_3,...,v_{n-1}-v_n,v_n,\\
&\text{es un subconjunto linealmente independiente de }V.
\end{aligned}subconjunto
}
\end{center}

\textbf{Demostración:}

Supongamos que existen coeficientes \( c_1, c_2, \ldots, c_n \) tales que:
\[ c_1(v_1-v_2) + c_2(v_2-v_3) + \ldots + c_{n-1}(v_{n-1}-v_n) + c_n v_n = \theta_V \]
Donde \( \theta_V \) es el vector cero en \( V \).

Expandiendo y agrupando términos, obtenemos:
\[ c_1 v_1 + (-c_1 + c_2) v_2 + \ldots + (-c_{n-1} + c_n) v_{n-1} + c_n v_n = \theta_V \]

Para que la combinación lineal anterior sea igual a \( \theta_V \), cada coeficiente de \( v_i \) debe ser cero. Dado que \( \{v_1, v_2, \ldots, v_n\} \) es linealmente independiente, todos los coeficientes \( c_1, c_2, \ldots, c_n \) deben ser cero.

En particular, \( c_n = 0 \) y \( -c_{n-1} + c_n = 0 \), lo que implica que \( c_{n-1} = 0 \). Continuando de esta manera, podemos concluir que todos los coeficientes \( c_1, c_2, \ldots, c_n \) son cero.

Por lo tanto, la lista \( v_1-v_2, v_2-v_3, \ldots, v_{n-1}-v_n, v_n \) es linealmente independiente en \( V \).

\[ \blacksquare \]


\boxed{
\begin{gathered}
\text{Sea }V\text{ un }K\text{-espacio vectorial. Supongamos que }\{v_1,...,v_n\}\text{ es un subconjunto} \\
\text{ linealmente independiente de }V\text{ y }w\in V.\text{ Prueba que }\{v_1,...,v_n,w\}\text{ es linealmente independiente  si y sólo si} \\
w\notin\langle v_1,...,v_n\rangle.
\end{gathered}
}


\textbf{Demostración:}

Para demostrar que el conjunto \( \{v_1,...,v_n,w\} \) es linealmente independiente si y sólo si \( w \notin \langle v_1,...,v_n \rangle \), vamos a demostrar ambas direcciones:

\textbf{1. \( \Rightarrow \) (Directa):} Supongamos que \( \{v_1,...,v_n,w\} \) es linealmente independiente y suponemos, por contradicción, que \( w \in \langle v_1,...,v_n \rangle \). 

Esto significa que \( w \) puede ser expresado como una combinación lineal de \( v_1,...,v_n \), es decir, existen coeficientes \( c_1,...,c_n \) tales que:
\[ w = c_1 v_1 + ... + c_n v_n \]

Si consideramos la combinación lineal:
\[ c_1 v_1 + ... + c_n v_n - w = 0 \]

Esto contradice nuestra suposición de que \( \{v_1,...,v_n,w\} \) es linealmente independiente, ya que no todos los coeficientes son cero. Por lo tanto, \( w \notin \langle v_1,...,v_n \rangle \).

\textbf{2. \( \Leftarrow \) (Recíproca):} Supongamos que \( w \notin \langle v_1,...,v_n \rangle \) y suponemos, por contradicción, que \( \{v_1,...,v_n,w\} \) es linealmente dependiente.

Esto significa que existen coeficientes \( c_1,...,c_n, d \), no todos cero, tales que:
\[ c_1 v_1 + ... + c_n v_n + d w = 0 \]

Dado que \( \{v_1,...,v_n\} \) es linealmente independiente, esto implica que \( d \neq 0 \). Por lo tanto, podemos expresar \( w \) como:
\[ w = -\frac{c_1}{d} v_1 - ... - \frac{c_n}{d} v_n \]

Esto contradice nuestra suposición de que \( w \notin \langle v_1,...,v_n \rangle \). Por lo tanto, \( \{v_1,...,v_n,w\} \) es linealmente independiente.

Hemos demostrado ambas direcciones, por lo que la afirmación es verdadera.

\[ \blacksquare \]

\end{document}
